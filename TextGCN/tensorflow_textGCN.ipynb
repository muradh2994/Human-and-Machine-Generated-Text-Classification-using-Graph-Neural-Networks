{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## This Model implementation is inspired from the GitHub Repository: https://github.com/yao8839836/text_gcn. I have appropriately modified the code for my usecase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y26etdQmBnOc",
        "outputId": "6055570c-8241-4f3b-cfaf-13b71ed02cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.47.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=cc67904c709d90cd4ac499a8a2a1d3218d5d3ac40814164745d477721e1815f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ],
      "source": [
        "#Install the tensorflow 1.15 version\r\n",
        "!pip install tensorflow==1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grw7WdxwCJWZ",
        "outputId": "e7ac4509-78a7-4b06-81a4-f216a1c2c581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ]
        }
      ],
      "source": [
        "# Check the tensorflow version\r\n",
        "import tensorflow\r\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVaQqiPmzYq7",
        "outputId": "19c7efaa-260b-4a01-ac2a-893f78ce411d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'text_gcn'...\n",
            "remote: Enumerating objects: 26801, done.\u001b[K\n",
            "remote: Total 26801 (delta 0), reused 0 (delta 0), pack-reused 26801\u001b[K\n",
            "Receiving objects: 100% (26801/26801), 861.51 MiB | 9.90 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "Checking out files: 100% (26397/26397), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone github repository\r\n",
        "!git clone https://github.com/yao8839836/text_gcn.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVwAG3GHLdbH",
        "outputId": "c8e13c7b-edb9-44ec-8522-c213daa07246"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download nltk library\r\n",
        "import nltk\r\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tak-6LIBxN2L",
        "outputId": "f23cd1f3-d03e-4faa-8217-3f0b3c12260b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/text_gcn\n"
          ]
        }
      ],
      "source": [
        "# change the working directory\r\n",
        "%cd /content/text_gcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TMjuLV-eLiA2"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset in the required format of the TextGCN model\r\n",
        "!python prepare_data.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrrKncDdxk2c",
        "outputId": "4b5baeef-11eb-461c-a503-5d5b38dbb99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'she', \"needn't\", 'if', 'so', 'don', 'has', 'only', 'myself', 're', 'against', \"don't\", 'am', 'doesn', 'by', \"mustn't\", 'our', 'further', 'that', 'wasn', 'each', 'will', 'here', 'ain', 'out', \"couldn't\", 'y', 'mustn', 'are', 'them', 'm', 'can', 'do', 'shouldn', 'the', 'under', 'own', 'hasn', 'other', 'as', 'after', 'most', 'was', 'yourselves', 'ma', 'isn', 'herself', 'who', \"it's\", 'both', 'it', 'to', 'these', 'same', \"haven't\", 'over', 'their', 'o', 't', 'his', 'ourselves', \"you'll\", 'is', 'in', \"doesn't\", 'where', 'been', 'those', 's', 'a', 'from', 'hadn', 'once', 'hers', 'weren', 'what', 'they', 'me', 'why', 'few', \"mightn't\", 'an', 'off', 'no', 'shan', \"should've\", 'won', 'were', 'such', \"isn't\", 'below', 'when', 'again', 'now', 'll', 'your', \"shouldn't\", 'yourself', 'couldn', 'you', 'or', 'yours', 'whom', 'down', 've', 'up', 'which', 'theirs', 'themselves', 'nor', 'just', 'then', 'all', \"hadn't\", 'being', 'on', 'above', 'but', 'my', 'did', 'before', 'aren', 'between', 'himself', 'than', \"weren't\", 'too', 'd', 'there', 'and', 'because', 'her', 'at', \"that'll\", \"you'd\", 'until', 'how', 'didn', 'ours', 'this', 'not', 'during', \"she's\", \"aren't\", 'he', \"hasn't\", 'him', 'be', 'any', \"you've\", \"didn't\", \"you're\", 'does', 'having', 'for', 'very', \"won't\", 'its', 'have', 'into', 'about', 'with', 'mightn', 'while', \"wouldn't\", 'itself', 'through', 'haven', 'i', 'should', 'had', 'some', 'wouldn', \"wasn't\", 'needn', 'more', 'we', 'doing', 'of', \"shan't\"}\n",
            "min_len : 1\n",
            "max_len : 523\n",
            "average_len : 231.573\n"
          ]
        }
      ],
      "source": [
        "# Remove stop words \r\n",
        "!python remove_words.py humanvsai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53COmGA_yKYJ",
        "outputId": "2aba2cfc-c431-474e-c20b-ef50a7333b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399]\n",
            "[1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999]\n",
            "[697, 852, 12, 527, 93, 2, 108, 1319, 323, 922, 702, 646, 18, 867, 245, 334, 375, 873, 536, 302, 735, 1037, 246, 345, 1330, 817, 484, 744, 177, 107, 1111, 241, 85, 65, 1232, 665, 1003, 842, 1099, 981, 314, 1375, 1221, 775, 405, 621, 663, 788, 647, 615, 191, 506, 581, 1002, 853, 643, 1358, 868, 743, 409, 1192, 571, 826, 497, 333, 742, 749, 95, 780, 415, 575, 625, 883, 754, 1390, 632, 1234, 709, 19, 1281, 436, 1310, 1054, 238, 237, 1279, 195, 1150, 97, 604, 221, 38, 1056, 595, 356, 472, 1101, 381, 464, 1194, 851, 591, 638, 215, 1227, 13, 1342, 906, 723, 655, 913, 511, 954, 366, 247, 620, 1206, 547, 970, 1302, 1303, 1083, 29, 1151, 49, 1059, 147, 812, 619, 1031, 840, 1296, 804, 205, 563, 785, 499, 608, 254, 1327, 152, 1230, 934, 653, 589, 965, 88, 1023, 363, 295, 187, 657, 1257, 1371, 1153, 944, 169, 441, 818, 514, 885, 1182, 990, 1089, 101, 380, 1258, 991, 393, 1026, 927, 510, 119, 808, 382, 809, 419, 315, 700, 26, 120, 958, 223, 607, 884, 561, 357, 388, 1088, 54, 242, 998, 943, 979, 679, 1197, 875, 87, 216, 1290, 576, 1096, 311, 908, 160, 539, 420, 317, 799, 475, 263, 423, 327, 224, 751, 758, 820, 331, 100, 1238, 480, 515, 776, 823, 1001, 640, 30, 257, 1295, 1246, 896, 17, 687, 1338, 1247, 457, 1291, 773, 1259, 513, 194, 902, 822, 434, 111, 78, 552, 372, 1200, 1325, 733, 1193, 813, 1191, 1311, 939, 603, 1280, 1341, 209, 1161, 22, 623, 1000, 84, 455, 1380, 307, 1069, 1125, 878, 1199, 600, 498, 310, 924, 1091, 1136, 72, 306, 925, 1347, 259, 1067, 508, 789, 1061, 869, 1361, 903, 548, 447, 1317, 1215, 535, 1152, 872, 950, 300, 467, 232, 1097, 829, 613, 1034, 162, 681, 1133, 747, 110, 1304, 135, 1276, 16, 698, 917, 618, 516, 816, 70, 312, 859, 1137, 768, 360, 1130, 379, 969, 255, 1032, 522, 693, 1082, 814, 1071, 624, 1115, 350, 234, 171, 588, 649, 1039, 1114, 650, 1164, 670, 167, 482, 62, 469, 703, 1398, 284, 24, 1049, 641, 838, 631, 769, 435, 1170, 21, 839, 1198, 309, 1122, 202, 118, 468, 193, 151, 590, 1269, 144, 1226, 76, 1006, 616, 128, 1332, 843, 1160, 947, 358, 949, 1320, 1245, 1047, 978, 1202, 899, 1351, 533, 674, 1370, 1286, 178, 930, 102, 340, 1116, 282, 1190, 612, 695, 857, 584, 55, 271, 753, 1218, 667, 73, 1397, 1265, 1074, 175, 1121, 1041, 37, 688, 639, 583, 661, 1102, 864, 412, 928, 1244, 287, 421, 1339, 347, 680, 1018, 301, 211, 1207, 849, 456, 305, 1313, 1183, 1387, 53, 668, 975, 567, 862, 248, 1070, 173, 528, 1029, 75, 127, 399, 80, 1264, 318, 901, 443, 1224, 1043, 1223, 373, 629, 525, 1337, 819, 1209, 418, 52, 131, 893, 1318, 233, 551, 1329, 389, 1243, 1357, 662, 543, 471, 564, 1030, 6, 518, 1391, 683, 921, 153, 269, 69, 504, 730, 678, 596, 825, 781, 692, 911, 1289, 989, 439, 1365, 1092, 656, 994, 1141, 774, 1142, 139, 1274, 554, 806, 569, 728, 352, 1168, 1189, 794, 231, 424, 648, 115, 795, 573, 3, 734, 134, 658, 546, 462, 1252, 404, 339, 1095, 121, 321, 519, 1155, 1177, 973, 77, 1134, 810, 1275, 952, 1382, 1262, 149, 450, 155, 1064, 23, 1027, 1098, 1386, 161, 112, 1383, 1139, 279, 57, 542, 1104, 1045, 1322, 1100, 1008, 791, 1169, 218, 459, 1345, 935, 686, 560, 634, 745, 964, 714, 1174, 598, 582, 148, 574, 1129, 1285, 895, 633, 606, 802, 593, 771, 1143, 133, 463, 1308, 230, 1301, 645, 288, 42, 125, 719, 1266, 1020, 1107, 186, 470, 1179, 1090, 1212, 319, 529, 1079, 1004, 807, 1186, 136, 64, 157, 1084, 1055, 184, 1204, 876, 453, 40, 1268, 891, 800, 253, 811, 429, 98, 1326, 627, 270, 1201, 832, 550, 1103, 996, 556, 1379, 166, 961, 628, 322, 755, 1278, 557, 955, 1261, 431, 1159, 1175, 392, 74, 433, 489, 297, 1057, 494, 881, 982, 559, 196, 992, 400, 1300, 265, 430, 1328, 338, 737, 395, 406, 931, 545, 664, 1229, 1062, 1025, 983, 1072, 1196, 15, 293, 361, 1128, 1333, 91, 1188, 391, 1185, 946, 299, 720, 68, 1388, 699, 1022, 385, 365, 476, 984, 887, 159, 1157, 782, 920, 1124, 601, 605, 198, 487, 1052, 916, 440, 767, 523, 1213, 8, 1106, 833, 458, 1135, 1222, 544, 863, 179, 701, 1250, 35, 675, 454, 1113, 500, 888, 1321, 485, 1355, 39, 390, 403, 910, 1108, 1117, 60, 188, 146, 236, 330, 761, 1242, 967, 426, 192, 779, 1312, 945, 394, 332, 654, 377, 368, 0, 204, 568, 336, 1315, 880, 344, 401, 277, 614, 66, 696, 1394, 422, 1324, 764, 586, 746, 1093, 997, 854, 408, 90, 206, 526, 1272, 298, 882, 792, 1362, 99, 630, 689, 239, 1350, 36, 1158, 694, 594, 1012, 1235, 845, 959, 1068, 690, 324, 731, 762, 1203, 190, 716, 411, 942, 407, 48, 1352, 478, 599, 1331, 562, 705, 31, 976, 778, 274, 442, 428, 1340, 651, 96, 383, 105, 103, 985, 1348, 617, 1277, 1094, 214, 956, 240, 207, 4, 384, 999, 727, 281, 250, 611, 677, 1208, 353, 572, 652, 1220, 659, 1368, 197, 948, 158, 164, 933, 313, 1110, 915, 898, 637, 289, 258, 937, 987, 870, 9, 1283, 736, 1126, 644, 217, 558, 673, 912, 1343, 1254, 1144, 1127, 1344, 837, 765, 33, 960, 213, 58, 622, 11, 871, 805, 490, 466, 341, 534, 370, 827, 292, 1035, 711, 1376, 1248, 369, 1051, 1156, 183, 889, 1040, 1036, 1058, 836, 900, 483, 1346, 505, 244, 995, 278, 82, 227, 721, 1354, 81, 444, 355, 460, 448, 387, 114, 1038, 1381, 1292, 1314, 750, 63, 264, 303, 1014, 1146, 503, 129, 549, 904, 294, 1210, 1334, 760, 538, 824, 276, 980, 461, 537, 521, 14, 918, 676, 685, 1299, 1147, 1205, 1228, 892, 1005, 156, 553, 1389, 798, 1165, 7, 725, 821, 325, 724, 1044, 726, 94, 1364, 367, 579, 966, 953, 41, 1270, 349, 481, 1373, 524, 473, 691, 116, 5, 185, 154, 335, 1033, 304, 1336, 609, 860, 890, 962, 602, 1077, 718, 642, 479, 577, 180, 163, 828, 50, 1086, 208, 739, 531, 897, 1288, 850, 143, 348, 446, 362, 1217, 1042, 565, 784, 988, 801, 1109, 1392, 671, 848, 1120, 585, 397, 786, 371, 59, 597, 1378, 398, 71, 1239, 713, 329, 974, 1249, 165, 1309, 941, 1187, 1323, 1073, 1112, 738, 414, 1009, 1145, 1087, 1384, 326, 1132, 174, 929, 296, 492, 210, 1393, 1293, 1263, 1, 138, 879, 1123, 1017, 320, 1019, 123, 1399, 763, 512, 1385, 1233, 708, 132, 847, 465, 261, 846, 938, 1349, 1166, 425, 1253, 706, 752, 932, 940, 1219, 919, 1080, 172, 855, 971, 501, 740, 226, 141, 704, 1060, 45, 914, 410, 1353, 488, 1118, 1011, 1075, 1335, 936, 1048, 113, 1176, 783, 894, 342, 796, 122, 660, 1271, 260, 1180, 328, 126, 47, 1173, 201, 1236, 517, 635, 926, 493, 32, 402, 729, 262, 427, 777, 86, 181, 756, 176, 130, 874, 43, 815, 1294, 951, 495, 1065, 1297, 252, 451, 831, 712, 1171, 710, 1007, 25, 432, 1081, 957, 757, 587, 530, 79, 858, 89, 290, 396, 189, 1231, 790, 203, 610, 907, 672, 666, 1078, 835, 438, 793, 1356, 67, 1154, 803, 145, 1178, 256, 417, 748, 56, 1148, 541, 364, 968, 268, 104, 378, 20, 717, 770, 636, 1214, 787, 83, 732, 866, 977, 1241, 92, 520, 266, 877, 182, 626, 1240, 1237, 540, 1184, 830, 491, 267, 682, 684, 1066, 222, 844, 1366, 570, 1360, 1284, 351, 1369, 886, 707, 308, 1085, 376, 1225, 502, 27, 1076, 856, 993, 1162, 285, 592, 1251, 1172, 1167, 283, 507, 416, 1021, 1028, 477, 199, 1105, 1316, 337, 437, 741, 474, 669, 124, 46, 1216, 963, 486, 346, 923, 1149, 10, 117, 354, 1024, 229, 219, 1372, 445, 496, 251, 1396, 249, 1267, 772, 273, 44, 170, 374, 106, 1138, 509, 1131, 200, 1374, 386, 1140, 51, 1256, 1377, 1273, 228, 359, 280, 1395, 909, 759, 28, 1298, 235, 865, 1013, 1010, 140, 291, 715, 1053, 905, 212, 1255, 841, 1015, 61, 1305, 722, 580, 1260, 834, 343, 861, 1211, 766, 1363, 797, 150, 1307, 225, 986, 168, 1063, 1359, 142, 1119, 413, 1016, 1046, 137, 272, 555, 1050, 1282, 243, 1306, 452, 1195, 578, 449, 34, 275, 1287, 566, 286, 532, 1181, 316, 220, 109, 1163, 972, 1367, 1681, 1625, 1532, 1927, 1520, 1859, 1745, 1531, 1735, 1672, 1608, 1960, 1506, 1540, 1509, 1465, 1807, 1802, 1427, 1496, 1430, 1436, 1958, 1500, 1821, 1621, 1827, 1699, 1637, 1995, 1933, 1876, 1971, 1718, 1477, 1616, 1478, 1472, 1945, 1645, 1812, 1682, 1526, 1928, 1498, 1611, 1632, 1902, 1483, 1961, 1748, 1930, 1843, 1732, 1684, 1501, 1600, 1853, 1888, 1779, 1515, 1584, 1950, 1749, 1640, 1973, 1734, 1628, 1697, 1783, 1759, 1786, 1773, 1747, 1679, 1691, 1497, 1551, 1838, 1485, 1772, 1913, 1828, 1422, 1428, 1998, 1700, 1523, 1578, 1762, 1962, 1931, 1424, 1910, 1547, 1673, 1461, 1883, 1664, 1448, 1947, 1544, 1882, 1603, 1511, 1643, 1948, 1824, 1784, 1708, 1419, 1648, 1968, 1499, 1522, 1692, 1439, 1794, 1742, 1952, 1403, 1991, 1698, 1896, 1743, 1569, 1517, 1493, 1977, 1819, 1934, 1630, 1530, 1617, 1816, 1425, 1678, 1431, 1869, 1589, 1769, 1785, 1610, 1983, 1954, 1880, 1727, 1631, 1959, 1932, 1633, 1441, 1796, 1850, 1815, 1863, 1666, 1598, 1845, 1516, 1687, 1791, 1752, 1641, 1978, 1712, 1852, 1922, 1635, 1571, 1433, 1597, 1619, 1432, 1879, 1609, 1582, 1539, 1661, 1521, 1583, 1674, 1923, 1565, 1456, 1872, 1574, 1729, 1897, 1851, 1411, 1937, 1717, 1893, 1943, 1488, 1831, 1919, 1489, 1967, 1660, 1758, 1587, 1753, 1846, 1438, 1642, 1980, 1505, 1647, 1940, 1804, 1541, 1585, 1442, 1889, 1542, 1774, 1733, 1695, 1546, 1849, 1594, 1705, 1406, 1723, 1644, 1460, 1655, 1656, 1903, 1480, 1561, 1990, 1414, 1866, 1974, 1792, 1834, 1686, 1841, 1555, 1527, 1874, 1704, 1813, 1918, 1826, 1895, 1915, 1590, 1987, 1676, 1443, 1997, 1606, 1780, 1894, 1793, 1800, 1720, 1886, 1667, 1634, 1801, 1939, 1709, 1412, 1818, 1914, 1781, 1771, 1941, 1711, 1693, 1669, 1925, 1966, 1741, 1777, 1426, 1519, 1907, 1650, 1605, 1949, 1875, 1942, 1722, 1690, 1854, 1938, 1721, 1607, 1764, 1453, 1985, 1994, 1409, 1715, 1810, 1576, 1554, 1754, 1462, 1797, 1468, 1559, 1825, 1789, 1417, 1884, 1714, 1719, 1533, 1946, 1822, 1766, 1429, 1911, 1556, 1524, 1564, 1512, 1553, 1475, 1844, 1502, 1658, 1595, 1663, 1702, 1976, 1767, 1649, 1407, 1878, 1507, 1573, 1970, 1495, 1652, 1677, 1936, 1538, 1965, 1626, 1572, 1508, 1466, 1688, 1768, 1402, 1421, 1847, 1873, 1420, 1737, 1435, 1696, 1440, 1806, 1908, 1775, 1694, 1514, 1898, 1740, 1979, 1706, 1972, 1842, 1400, 1657, 1836, 1492, 1665, 1885, 1405, 1920, 1716, 1484, 1671, 1881, 1926, 1951, 1809, 1444, 1906, 1474, 1837, 1476, 1469, 1618, 1534, 1592, 1856, 1550, 1636, 1622, 1765, 1418, 1982, 1989, 1445, 1535, 1770, 1909, 1963, 1454, 1602, 1452, 1579, 1929, 1868, 1651, 1855, 1404, 1639, 1629, 1701, 1992, 1604, 1487, 1548, 1905, 1566, 1917, 1757, 1988, 1830, 1580, 1654, 1563, 1728, 1471, 1486, 1415, 1993, 1808, 1975, 1423, 1494, 1623, 1703, 1612, 1755, 1413, 1627, 1463, 1689, 1668, 1829, 1659, 1871, 1653, 1912, 1798, 1545, 1416, 1921, 1560, 1788, 1504, 1867, 1638, 1996, 1552, 1877, 1823, 1750, 1862, 1787, 1984, 1817, 1840, 1680, 1957, 1458, 1591, 1593, 1739, 1670, 1857, 1744, 1870, 1944, 1956, 1446, 1490, 1953, 1525, 1577, 1999, 1803, 1900, 1710, 1567, 1675, 1613, 1832, 1820, 1624, 1599, 1776, 1529, 1725, 1746, 1410, 1447, 1839, 1887, 1451, 1935, 1761, 1434, 1510, 1479, 1892, 1805, 1562, 1558, 1455, 1860, 1864, 1865, 1731, 1713, 1596, 1924, 1890, 1782, 1707, 1790, 1481, 1568, 1473, 1955, 1899, 1614, 1581, 1730, 1482, 1683, 1964, 1464, 1437, 1969, 1858, 1724, 1904, 1738, 1861, 1760, 1646, 1588, 1457, 1901, 1557, 1537, 1459, 1543, 1726, 1778, 1685, 1662, 1549, 1848, 1513, 1799, 1518, 1586, 1795, 1891, 1835, 1814, 1736, 1751, 1620, 1528, 1536, 1916, 1575, 1763, 1981, 1467, 1986, 1491, 1401, 1756, 1601, 1833, 1570, 1470, 1450, 1615, 1811, 1503, 1449, 1408]\n",
            "2000\n",
            "Loaded Word Vectors!\n",
            "[[0 1 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 1 0]\n",
            " ...\n",
            " [0 0 1 0]\n",
            " [0 0 1 0]\n",
            " [0 0 0 1]]\n",
            "[[0 0 1 0]\n",
            " [0 0 0 1]\n",
            " [1 0 0 0]\n",
            " ...\n",
            " [1 0 0 0]\n",
            " [1 0 0 0]\n",
            " [0 0 0 1]]\n",
            "(1260, 1000) (1260, 4) (600, 1000) (600, 4) (9619, 1000) (9619, 4)\n"
          ]
        }
      ],
      "source": [
        "# Build graph using our 'huamnvsai' dataset\r\n",
        "!python build_graph.py humanvsai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHaULHSFykXV",
        "outputId": "1f0ad904-58f1-4619-efc7-bd37b89c7f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From train.py:27: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "(1260, 1000) (1260, 4) (600, 1000) (600, 4) (9619, 1000) (9619, 4)\n",
            "10219\n",
            "  (0, 1430)\t1.0876723486297752\n",
            "  (0, 1446)\t5.162298599615154\n",
            "  (0, 1453)\t4.61519526963518\n",
            "  (0, 1485)\t2.4476108650443034\n",
            "  (0, 1522)\t4.1821822709256695\n",
            "  (0, 1646)\t3.079113882493042\n",
            "  (0, 1707)\t4.2336066295556085\n",
            "  (0, 1739)\t7.223836825955616\n",
            "  (0, 1757)\t3.540459448995663\n",
            "  (0, 1770)\t2.873514640829742\n",
            "  (0, 1789)\t4.045554398052669\n",
            "  (0, 1813)\t4.509860006183766\n",
            "  (0, 1847)\t3.3104430183936913\n",
            "  (0, 1871)\t1.4148147318381283\n",
            "  (0, 1890)\t3.1820618517454844\n",
            "  (0, 1953)\t5.115995809754082\n",
            "  (0, 1984)\t2.2585682075772713\n",
            "  (0, 2004)\t3.473768074496991\n",
            "  (0, 2012)\t4.8283137373023015\n",
            "  (0, 2030)\t4.710530701645918\n",
            "  (0, 2085)\t5.298317366548036\n",
            "  (0, 2164)\t1.5394455406140655\n",
            "  (0, 2171)\t1.4167535686045991\n",
            "  (0, 2190)\t5.318520073865556\n",
            "  (0, 2191)\t2.2203578500791505\n",
            "  :\t:\n",
            "  (10218, 8377)\t2.184802057337662\n",
            "  (10218, 8412)\t1.5511690043101247\n",
            "  (10218, 8452)\t5.115995809754082\n",
            "  (10218, 8499)\t0.5276327420823719\n",
            "  (10218, 8663)\t5.035953102080546\n",
            "  (10218, 8804)\t3.3426266323043756\n",
            "  (10218, 8874)\t5.809142990314028\n",
            "  (10218, 8890)\t1.3093333199837622\n",
            "  (10218, 8921)\t1.4085399700672103\n",
            "  (10218, 8965)\t5.8375424648357255\n",
            "  (10218, 8983)\t1.246532418744732\n",
            "  (10218, 8995)\t3.989984546897858\n",
            "  (10218, 9056)\t0.6115671935675224\n",
            "  (10218, 9104)\t1.6220166946409607\n",
            "  (10218, 9239)\t2.985781942700823\n",
            "  (10218, 9281)\t3.754634715179403\n",
            "  (10218, 9304)\t6.762789508731951\n",
            "  (10218, 9398)\t4.451248103715835\n",
            "  (10218, 9427)\t3.3524072174927233\n",
            "  (10218, 9430)\t2.2585682075772713\n",
            "  (10218, 9444)\t1.0010319603292457\n",
            "  (10218, 9445)\t2.896339529675956\n",
            "  (10218, 9492)\t3.5935692743096115\n",
            "  (10218, 9535)\t2.821778966430553\n",
            "  (10218, 9568)\t3.338222582500767\n",
            "(10219, 10219)\n",
            "(10219, 10219)\n",
            "WARNING:tensorflow:From train.py:77: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:79: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:81: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "10219\n",
            "WARNING:tensorflow:From /content/text_gcn/models.py:143: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/models.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/inits.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/layers.py:82: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/layers.py:26: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/text_gcn/models.py:52: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/text_gcn/models.py:52: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "Tensor(\"graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0\", shape=(?, 4), dtype=float32)\n",
            "WARNING:tensorflow:From /content/text_gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From train.py:91: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:91: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-08-25 20:46:38.665552: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-08-25 20:46:38.670224: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2022-08-25 20:46:38.670541: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29f7640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-08-25 20:46:38.670585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-08-25 20:46:38.672505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-08-25 20:46:38.707884: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-08-25 20:46:38.707935: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 9e46aa67d6e4\n",
            "2022-08-25 20:46:38.707951: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 9e46aa67d6e4\n",
            "2022-08-25 20:46:38.708035: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3\n",
            "2022-08-25 20:46:38.708072: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3\n",
            "2022-08-25 20:46:38.708087: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3\n",
            "WARNING:tensorflow:From train.py:105: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Epoch: 0001 train_loss= 1.38627 train_acc= 0.24048 val_loss= 1.35959 val_acc= 0.25000 time= 1.24998\n",
            "Epoch: 0002 train_loss= 1.35820 train_acc= 0.25794 val_loss= 1.31902 val_acc= 0.25000 time= 1.11708\n",
            "Epoch: 0003 train_loss= 1.31371 train_acc= 0.26190 val_loss= 1.26829 val_acc= 0.26429 time= 1.17494\n",
            "Epoch: 0004 train_loss= 1.26001 train_acc= 0.27540 val_loss= 1.21167 val_acc= 0.29286 time= 1.13577\n",
            "Epoch: 0005 train_loss= 1.20219 train_acc= 0.29603 val_loss= 1.15299 val_acc= 0.32857 time= 1.15237\n",
            "Epoch: 0006 train_loss= 1.13959 train_acc= 0.33651 val_loss= 1.09163 val_acc= 0.38571 time= 1.15234\n",
            "Epoch: 0007 train_loss= 1.07796 train_acc= 0.39365 val_loss= 1.02515 val_acc= 0.46429 time= 1.14731\n",
            "Epoch: 0008 train_loss= 1.00720 train_acc= 0.45317 val_loss= 0.95658 val_acc= 0.50000 time= 1.14839\n",
            "Epoch: 0009 train_loss= 0.93749 train_acc= 0.49444 val_loss= 0.89274 val_acc= 0.59286 time= 1.14003\n",
            "Epoch: 0010 train_loss= 0.86903 train_acc= 0.60794 val_loss= 0.83608 val_acc= 0.65000 time= 1.14413\n",
            "Epoch: 0011 train_loss= 0.81378 train_acc= 0.68889 val_loss= 0.78336 val_acc= 0.66429 time= 1.13151\n",
            "Epoch: 0012 train_loss= 0.76384 train_acc= 0.70555 val_loss= 0.73216 val_acc= 0.67143 time= 1.13918\n",
            "Epoch: 0013 train_loss= 0.71390 train_acc= 0.71270 val_loss= 0.68483 val_acc= 0.70000 time= 1.14973\n",
            "Epoch: 0014 train_loss= 0.66318 train_acc= 0.73809 val_loss= 0.64520 val_acc= 0.72143 time= 1.14578\n",
            "Epoch: 0015 train_loss= 0.62222 train_acc= 0.77063 val_loss= 0.61308 val_acc= 0.72857 time= 1.13758\n",
            "Epoch: 0016 train_loss= 0.59977 train_acc= 0.78809 val_loss= 0.58688 val_acc= 0.72143 time= 1.17412\n",
            "Epoch: 0017 train_loss= 0.56201 train_acc= 0.81032 val_loss= 0.56651 val_acc= 0.74286 time= 1.14541\n",
            "Epoch: 0018 train_loss= 0.51292 train_acc= 0.84603 val_loss= 0.54694 val_acc= 0.74286 time= 1.13976\n",
            "Epoch: 0019 train_loss= 0.51173 train_acc= 0.83730 val_loss= 0.52798 val_acc= 0.76429 time= 1.14934\n",
            "Epoch: 0020 train_loss= 0.49665 train_acc= 0.83730 val_loss= 0.51149 val_acc= 0.75000 time= 1.13657\n",
            "Epoch: 0021 train_loss= 0.46409 train_acc= 0.84048 val_loss= 0.49632 val_acc= 0.75714 time= 1.13350\n",
            "Epoch: 0022 train_loss= 0.44356 train_acc= 0.84048 val_loss= 0.48221 val_acc= 0.75000 time= 1.15961\n",
            "Epoch: 0023 train_loss= 0.42961 train_acc= 0.84127 val_loss= 0.46981 val_acc= 0.75000 time= 1.14870\n",
            "Epoch: 0024 train_loss= 0.41029 train_acc= 0.85397 val_loss= 0.45967 val_acc= 0.76429 time= 1.15265\n",
            "Epoch: 0025 train_loss= 0.38031 train_acc= 0.86587 val_loss= 0.44955 val_acc= 0.75714 time= 1.14745\n",
            "Epoch: 0026 train_loss= 0.37949 train_acc= 0.85952 val_loss= 0.44103 val_acc= 0.76429 time= 1.14414\n",
            "Epoch: 0027 train_loss= 0.38205 train_acc= 0.85794 val_loss= 0.43567 val_acc= 0.76429 time= 1.14386\n",
            "Epoch: 0028 train_loss= 0.35610 train_acc= 0.86270 val_loss= 0.43120 val_acc= 0.74286 time= 1.12873\n",
            "Epoch: 0029 train_loss= 0.35123 train_acc= 0.86349 val_loss= 0.42516 val_acc= 0.73571 time= 1.13913\n",
            "Epoch: 0030 train_loss= 0.33333 train_acc= 0.86984 val_loss= 0.41775 val_acc= 0.75000 time= 1.13606\n",
            "Epoch: 0031 train_loss= 0.32695 train_acc= 0.86984 val_loss= 0.41191 val_acc= 0.75714 time= 1.14322\n",
            "Epoch: 0032 train_loss= 0.32106 train_acc= 0.87301 val_loss= 0.40793 val_acc= 0.76429 time= 1.16510\n",
            "Epoch: 0033 train_loss= 0.30300 train_acc= 0.87301 val_loss= 0.40445 val_acc= 0.75714 time= 1.14848\n",
            "Epoch: 0034 train_loss= 0.29864 train_acc= 0.87778 val_loss= 0.40164 val_acc= 0.75714 time= 1.16350\n",
            "Epoch: 0035 train_loss= 0.28784 train_acc= 0.87540 val_loss= 0.39962 val_acc= 0.75714 time= 1.15906\n",
            "Epoch: 0036 train_loss= 0.28362 train_acc= 0.88254 val_loss= 0.39837 val_acc= 0.75714 time= 1.14066\n",
            "Epoch: 0037 train_loss= 0.27795 train_acc= 0.87540 val_loss= 0.39533 val_acc= 0.77143 time= 1.15197\n",
            "Epoch: 0038 train_loss= 0.26856 train_acc= 0.88016 val_loss= 0.39313 val_acc= 0.77143 time= 1.16148\n",
            "Epoch: 0039 train_loss= 0.26021 train_acc= 0.89444 val_loss= 0.39178 val_acc= 0.77143 time= 1.19761\n",
            "Epoch: 0040 train_loss= 0.25341 train_acc= 0.90000 val_loss= 0.39102 val_acc= 0.76429 time= 1.15238\n",
            "Epoch: 0041 train_loss= 0.24806 train_acc= 0.89762 val_loss= 0.39072 val_acc= 0.76429 time= 1.17680\n",
            "Epoch: 0042 train_loss= 0.24176 train_acc= 0.90397 val_loss= 0.39068 val_acc= 0.76429 time= 1.16592\n",
            "Epoch: 0043 train_loss= 0.23668 train_acc= 0.90317 val_loss= 0.39017 val_acc= 0.76429 time= 1.15218\n",
            "Epoch: 0044 train_loss= 0.22843 train_acc= 0.91270 val_loss= 0.38932 val_acc= 0.77143 time= 1.15439\n",
            "Epoch: 0045 train_loss= 0.22225 train_acc= 0.90952 val_loss= 0.38882 val_acc= 0.77143 time= 1.14981\n",
            "Epoch: 0046 train_loss= 0.22262 train_acc= 0.91825 val_loss= 0.38878 val_acc= 0.77143 time= 1.12945\n",
            "Epoch: 0047 train_loss= 0.21563 train_acc= 0.92063 val_loss= 0.38910 val_acc= 0.78571 time= 1.15038\n",
            "Early stopping...\n",
            "Optimization Finished!\n",
            "Test set results: cost= 0.33100 accuracy= 0.83667 time= 0.41061\n",
            "10219\n",
            "Test Precision, Recall and F1-Score...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.9763    0.9880       169\n",
            "           1     0.9737    1.0000    0.9867       148\n",
            "           2     0.6479    0.6765    0.6619       136\n",
            "           3     0.6879    0.6599    0.6736       147\n",
            "\n",
            "    accuracy                         0.8367       600\n",
            "   macro avg     0.8274    0.8282    0.8275       600\n",
            "weighted avg     0.8372    0.8367    0.8367       600\n",
            "\n",
            "Macro average Test Precision, Recall and F1-Score...\n",
            "(0.8273786992203314, 0.8281664736900678, 0.8275430583676772, None)\n",
            "Micro average Test Precision, Recall and F1-Score...\n",
            "(0.8366666666666667, 0.8366666666666667, 0.8366666666666667, None)\n",
            "\n",
            "\n",
            "Accuracy and Loss Curve\n",
            "Figure(1000x500)\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the model\r\n",
        "!python train.py humanvsai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWcGlt3Q4TAg",
        "outputId": "14e582a4-a1ae-4c54-b6a4-6cdac1887fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "Figure(640x480)\n"
          ]
        }
      ],
      "source": [
        "# Visualize the results\r\n",
        "!python visualize.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zhGNjylyfpJ",
        "outputId": "4793398b-959e-4957-982b-a048931d2252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "Figure(640x480)\n"
          ]
        }
      ],
      "source": [
        "!python visualize_words.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bERNOMPJzsEi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "tensorflow_textGCN.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}